---
title: "IGN Video Game Reviews"
author: "by Ellen A Savoye"
date: "July 11, 2018"
output: pdf_document
subtitle: Machine Learning
---
***

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      echo=FALSE, warning=FALSE, message=FALSE)
```

```{r game_reviews data, echo=FALSE, include=FALSE}

library("knitr")
library("readr")
library("dplyr")
library("tidyr")
library("RColorBrewer")
library("ggplot2")
library("caTools")
library("randomForest")
library("rpart")
library("rpart.plot")
library(caret)

IGN_data_cleaned <- read.csv("ign_clean.csv")

IGN_data_cleaned <- IGN_data_cleaned[,-c(1,4)]
```

# Predictive Models

#### Subsetting the Data

Prior to any ventures into modeling, a data set was created to remove any unnecessary features and any blank records. 36 records with a 'blank' genre were removed. Given that some of the kept features are categorical, a conversion to binary/dummy variables was needed using model matrix. I chose to not combine release day, month, and year into a combined date field as I don't believe it would have given any valuable insight as unique a combined value. 

```{r subset_data, tidy=TRUE}

sapply(IGN_data_cleaned, function(x) sum(is.na(x)))
IGN_DC_PredM_WONA <- na.omit(IGN_data_cleaned)
IGN_DC_PredM <- IGN_DC_PredM_WONA[, c(-1, -2, -3, -5)]

#Let convert time related variable to factor so we can binarize them
IGN_DC_PredM$release_day   <- as.factor(IGN_DC_PredM$release_day)
IGN_DC_PredM$release_month <- as.factor(IGN_DC_PredM$release_month)
IGN_DC_PredM$release_year  <- as.factor(IGN_DC_PredM$release_year)

str(IGN_DC_PredM)

IGN_MM <- model.matrix(~ editors_choice + platform_group + genre_group + release_day + release_month + release_day, data=IGN_DC_PredM)

dim(IGN_MM)

#drop near zero predictor
nzr <- nearZeroVar(IGN_MM)
nzr
IGN_MM <- IGN_MM[, -nzr]

dim(IGN_MM)

#drop highly correlated features
corel <- cor(IGN_MM)
highCorrel <- findCorrelation(corel, cutoff = 0.85) #no correlation above 85%. Great!
highCorrel

#Now let's combine our data for regression analysis.
reg_data <- cbind(IGN_MM, score = IGN_DC_PredM$score)

Hmisc::describe(reg_data)
```

#### Linear Regression

```{r sig_features, tidy=TRUE}
#convert our matrix data to a data frame for ease of use
reg_data <- as.data.frame(reg_data)

#make sure column names are compatible with R naming convention
colnames(reg_data) <- make.names(colnames(reg_data))

ScoreReg <- lm(score ~., data = reg_data)
summary(ScoreReg)

reg_data$platform_groupApple <- NULL
reg_data$genre_groupAdventure <- NULL
reg_data$genre_groupSports <- NULL
reg_data$release_month5 <- NULL
reg_data$release_month12 <- NULL

ScoreReg <- lm(score ~., data = reg_data)
summary(ScoreReg)

reg_data$genre_groupShooter <- NULL
reg_data$release_month7 <- NULL

ScoreReg <- lm(score ~., data = reg_data)
summary(ScoreReg)

```

The first iteration of the linear regression showed multicollinearity between score and score phrase with a multiple R-squared of 0.9726. After removing score_phrase, R-squared drops to 0.3187. In an attempt to improve R-squared, near zero variance predictors (NZP) were identified and removed because they are non-informative and tend to occur when breaking categorical variables into dummy variables, as was done above. After removing NZP and checking for high collinearity, a few iterations of the linear model were run in order to narrow down which features are not significant. A few of the fields removed are platform_groupApple, genre_groupAdventure, and genre_groupSports. After removing non-significant fields, the adjusted R-squared is now 0.3127. 

#### CART/Random Forest

```{r random_forest, tidy=TRUE}

set.seed(1000)

spl <- sample.split(reg_data$score, SplitRatio = 0.8)
Train <- subset(reg_data, spl == TRUE)
Test <- subset(reg_data, spl == FALSE)

# CART
IGN_Tree <- rpart(score ~., data = Train, method = "class", minbucket = 25)
prp(IGN_Tree)

IGN_Tree_v2 <- rpart(score ~., data = Train, method = "class", control = rpart.control(minbucket = 25, cp = 0.0025))
prp(IGN_Tree_v2)

# Random Forest
IGN_Forest <- randomForest(score ~., data = Train, ntree = 500)

IGN_Forest

preds_f <- predict(IGN_Forest, Test)
#R-square of the final RF model -> 32.69 %
cor(preds_f, Test$score)^2

preds_t <- predict(IGN_Tree, Test)
#R-square of the final 1 node Tree model -> ~30.4 %
cor(preds_t, Test$score)^2

preds_t_v2 <- predict(IGN_Tree_v2, Test)
#R-square of the final multi-node Tree model -> ~30.4 %
cor(preds_t_v2, Test$score)^2

```

When looking at the first tree, the only node shown is for editor's choice. If editor's choice is greater than 0.5, it is no or 1. One important note is that a high score can imply an editor's choice designation. However, a high score, say 8 or greater, does not automatically receive an editor's choice designation. Using the complexity parameter (cp), we can force more nodes to appear with cp = .0025. Looking at the tree, we see that editor's choice has remained as the first node. The first tree produces an R-squared of 30.4% which is similar to the linear regression model. When forcing the tree to have multiple nodes, the same R-squared calculation doesn't work as nicely nor is it the most efficient.

The random forest model, built on the training dataset, appears to be the best model based on a comparison of R-squared at 32.69% versus 30.4% (CART), and 31.27% (linear regression).